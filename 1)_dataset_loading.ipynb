{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6498c34a11e1ab7",
   "metadata": {},
   "source": [
    "## Installation des packages nécessaires\n",
    "\n",
    "- s3prl : framework d'extraction de features audio\n",
    "- soundfile : lecture/écriture de fichiers audio\n",
    "- torchaudio : traitement audio avec PyTorch\n",
    "- librosa : traitement audio\n",
    "- tqdm : barre de progression"
   ]
  },
  {
   "cell_type": "code",
   "id": "9120f260cc98b062",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:25:36.146815Z",
     "start_time": "2025-11-21T17:25:36.143978Z"
    }
   },
   "source": [
    "# %pip install --upgrade pip setuptools wheel\n",
    "#\n",
    "# %pip install s3prl --upgrade\n",
    "# %pip install huggingface_hub -- upgrade\n",
    "#\n",
    "# %pip install soundfile --upgrade\n",
    "# %pip install torchaudio --upgrade\n",
    "# %pip install librosa --upgrade\n",
    "#\n",
    "# %pip install tqdm --upgrade\n",
    "# %pip install scikit-learn --upgrade\n",
    "# %pip install matplotlib --upgrade"
   ],
   "outputs": [],
   "execution_count": 62
  },
  {
   "cell_type": "markdown",
   "id": "7fa1ee19d8ccb6c3",
   "metadata": {},
   "source": [
    "## Import des packages\n",
    "\n",
    "- tqdm : barre de progression\n",
    "- pathlib : gestion des chemins de fichiers\n",
    "- librosa : traitement audio\n",
    "- os : gestion des fichiers\n",
    "- torch : PyTorch\n",
    "- numpy : calculs numériques\n",
    "- soundfile : lecture/écriture de fichiers audio\n",
    "- s3prl : framework d'extraction de features audio\n",
    "- sklearn : outils de machine learning"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-21T17:25:36.152671Z",
     "start_time": "2025-11-21T17:25:36.150299Z"
    }
   },
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from s3prl.nn import S3PRLUpstream\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 63
  },
  {
   "cell_type": "markdown",
   "id": "2d3e0a58453eef4a",
   "metadata": {},
   "source": [
    "## Paramètres globaux\n",
    "\n",
    "- random_seed : graine aléatoire pour reproductibilité\n",
    "- device : choix du device (GPU si disponible, sinon CPU)\n",
    "- dataset_path : chemin vers le dataset audio original\n",
    "- audio_path : chemin vers le dossier de sauvegarde des audios rééchantillonnés\n",
    "- embedding_path : chemin vers le dossier de sauvegarde des embeddings extraits"
   ]
  },
  {
   "cell_type": "code",
   "id": "b9f0a99d81d5d36e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:25:36.162141Z",
     "start_time": "2025-11-21T17:25:36.158366Z"
    }
   },
   "source": [
    "random_seed = 42\n",
    "limit = 200\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "dataset_path = Path(\"./dataset/\")\n",
    "audio_path = Path(\"./16k_Hz_audio/\")\n",
    "embedding_path = Path(\"./16k_Hz_embedding/\")\n",
    "pooled_path = Path(\"./16k_Hz_pooled/\")\n",
    "\n",
    "# Pooled versions paths\n",
    "pooled_mean_path = pooled_path / \"mean\"\n",
    "pooled_max_path = pooled_path / \"max\"\n",
    "pooled_mean_std_path = pooled_path / \"mean_std\"\n",
    "\n",
    "train_mean_path = pooled_mean_path / \"_train.pt\"\n",
    "val_mean_path = pooled_mean_path / \"_val.pt\"\n",
    "test_mean_path = pooled_mean_path / \"_test.pt\"\n",
    "\n",
    "train_max_path = pooled_max_path / \"_train.pt\"\n",
    "val_max_path = pooled_max_path / \"_val.pt\"\n",
    "test_max_path = pooled_max_path / \"_test.pt\"\n",
    "\n",
    "train_mean_std_path = pooled_mean_std_path / \"_train.pt\"\n",
    "val_mean_std_path = pooled_mean_std_path / \"_val.pt\"\n",
    "test_mean_std_path = pooled_mean_std_path / \"_test.pt\"\n",
    "\n",
    "# Not pooled versions paths\n",
    "transformer_path = embedding_path / \"transformer\"\n",
    "\n",
    "train_transformer_path = transformer_path / \"_train.pt\"\n",
    "val_transformer_path = transformer_path / \"_val.pt\"\n",
    "test_transformer_path = transformer_path / \"_test.pt\"\n",
    "\n",
    "audio_path.mkdir(exist_ok=True)\n",
    "embedding_path.mkdir(exist_ok=True)\n",
    "pooled_path.mkdir(exist_ok=True)\n",
    "pooled_mean_path.mkdir(exist_ok=True)\n",
    "pooled_max_path.mkdir(exist_ok=True)\n",
    "pooled_mean_std_path.mkdir(exist_ok=True)\n",
    "transformer_path.mkdir(exist_ok=True)\n",
    "\n",
    "AUDIO_EXTENSIONS = (\".wav\", \".mp3\", \".flac\", \".ogg\", \".m4a\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "cell_type": "markdown",
   "id": "64fe7205863acfc7",
   "metadata": {},
   "source": [
    "## Fonctions utilitaires pour associer les fichiers audio à leurs émotions\n",
    "- find_emotion_T : fonction pour la base TESS\n",
    "- emotionfix : fonction pour ajuster les indices d'émotions pour la classification"
   ]
  },
  {
   "cell_type": "code",
   "id": "28f28b1b49bd84a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:25:36.170320Z",
     "start_time": "2025-11-21T17:25:36.167751Z"
    }
   },
   "source": [
    "# Emotion kind validation function for TESS database, due to emotions written within the file names.\n",
    "def find_emotion_T(name):\n",
    "    if 'neutral' in name:\n",
    "        return \"01\"\n",
    "    elif 'happy' in name or 'joy' in name or 'positive' in name:\n",
    "        return \"03\"\n",
    "    elif 'sad' in name or 'sadness' in name or 'pain' in name:\n",
    "        return \"04\"\n",
    "    elif 'angry' in name or 'anger' in name:\n",
    "        return \"05\"\n",
    "    elif 'fear' in name:\n",
    "        return \"06\"\n",
    "    elif 'disgust' in name or 'negative' in name:\n",
    "        return \"07\"\n",
    "    elif 'ps' in name or 'surprise' in name:\n",
    "        return \"08\"\n",
    "    else:\n",
    "        return \"-1\"\n",
    "\n",
    "\n",
    "# 'emotions' list fix for classification purposes:\n",
    "#     Classification values start from 0, Thus an 'n = n-1' operation has been executed for both RAVDESS and TESS databases:\n",
    "def emotionfix(e_num):\n",
    "    if e_num == \"01\":\n",
    "        return 0  # neutral\n",
    "    elif e_num == \"02\":\n",
    "        return 1  # calm\n",
    "    elif e_num == \"03\":\n",
    "        return 2  # happy\n",
    "    elif e_num == \"04\":\n",
    "        return 3  # sad\n",
    "    elif e_num == \"05\":\n",
    "        return 4  # angry\n",
    "    elif e_num == \"06\":\n",
    "        return 5  # fear\n",
    "    elif e_num == \"07\":\n",
    "        return 6  # disgust\n",
    "    else:\n",
    "        return 7  # suprised\n",
    "\n",
    "labels = [0,1,2,3,4,5,6,7]"
   ],
   "outputs": [],
   "execution_count": 65
  },
  {
   "cell_type": "markdown",
   "id": "c392b0d57cfe08c4",
   "metadata": {},
   "source": [
    "## Rééchantillonnage des fichiers audio à 16kHz et sauvegarde\n",
    "- On parcourt le dataset original\n",
    "- Si l'échantillonnage n'est pas à 16kHz\n",
    "  - On rééchantillonne chaque fichier à 16kHz si nécessaire\n",
    "  - On sauvegarde les fichiers rééchantillonnés dans un nouveau dossier\n",
    "- Sinon\n",
    "  - On sauvegarde le fichier"
   ]
  },
  {
   "cell_type": "code",
   "id": "abbe6d723755288c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:25:36.230257Z",
     "start_time": "2025-11-21T17:25:36.175948Z"
    }
   },
   "source": [
    "sample_rate = 16000\n",
    "nb_treated_file = 0\n",
    "fnb = 0\n",
    "\n",
    "for subdir, dirs, files in tqdm(os.walk(dataset_path), desc=\"Re-sampling audio files\"):\n",
    "    if not \"emotions\" in str(subdir):\n",
    "\n",
    "        for file in files:\n",
    "            if file.lower().endswith(AUDIO_EXTENSIONS):\n",
    "\n",
    "                # Ecriture du nouveau nom de fichier\n",
    "                save_path = audio_path / file\n",
    "\n",
    "                if not save_path.exists():\n",
    "\n",
    "                    nb_treated_file += 1\n",
    "\n",
    "                    y, sr = librosa.load(Path(subdir) / file, sr=None)  # sr=None garde la fréquence originale\n",
    "\n",
    "                    if sr != sample_rate:\n",
    "                        # Re-échantillonner\n",
    "                        y_resampled = librosa.resample(y, orig_sr=sr, target_sr=sample_rate)\n",
    "\n",
    "                        # Sauvegarder\n",
    "                        sf.write(save_path, y_resampled, sample_rate)\n",
    "                    else:\n",
    "                        sf.write(save_path, y, sample_rate)\n",
    "\n",
    "\n",
    "            fnb += 1\n",
    "print(f\"Number of files re-sampled and saved: {nb_treated_file}\")\n",
    "print(f\"Total number of files : {fnb}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Re-sampling audio files: 51it [00:00, 1004.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files re-sampled and saved: 0\n",
      "Total number of files : 4240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "cell_type": "markdown",
   "id": "8efb2a4629607dc8",
   "metadata": {},
   "source": [
    "## Load du modèle upstream WavLM via S3PRL et extraction des embeddings\n",
    "- On liste les modèles disponibles via torch.hub\n",
    "- On choisit le modèle WavLM (large de préférence)\n",
    "- On charge le modèle upstream (téléchargement si nécessaire)"
   ]
  },
  {
   "cell_type": "code",
   "id": "1a07282d05e27b94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:25:42.121405Z",
     "start_time": "2025-11-21T17:25:36.233587Z"
    }
   },
   "source": [
    "# 1) lister les modèles exposés via torch.hub (optionnel mais utile)\n",
    "print(\"Listing available torch.hub upstreal models from s3prl (may take 1-2s)...\")\n",
    "try:\n",
    "    hub_list = torch.hub.list('s3prl/s3prl')\n",
    "    print(\"Some available models (first 30):\\n\", hub_list[:30])\n",
    "except Exception as e:\n",
    "    print(\"Impossible de lister via torch.hub:\", e)\n",
    "    hub_list = []\n",
    "\n",
    "# 2) choisir un upstream (essaye wavlm_large, sinon wavlm_base_plus, sinon 'wavlm' puis tu pourras adapter)\n",
    "chosen = \"wavlm_large\"\n",
    "print(f\"\\nWill try to load upstream name: {chosen}\")\n",
    "\n",
    "# 3) load the upstream model (this will download checkpoint if needed)\n",
    "upstream_model = S3PRLUpstream(chosen)\n",
    "upstream_model.to(device)\n",
    "upstream_model.eval()\n",
    "print(\"Loaded upstream:\", chosen)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing available torch.hub upstreal models from s3prl (may take 1-2s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/alves/.cache/torch/hub/s3prl_s3prl_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some available models (first 30):\n",
      " ['FileLock', 'Path', 'apc', 'apc_360hr', 'apc_960hr', 'apc_local', 'apc_url', 'ast', 'audio_albert', 'audio_albert_960hr', 'audio_albert_local', 'audio_albert_logMelBase_T_share_AdamW_b32_1m_960hr_drop1', 'audio_albert_url', 'baseline', 'baseline_local', 'byol_a_1024', 'byol_a_2048', 'byol_a_512', 'byol_s_cvt', 'byol_s_default', 'byol_s_resnetish34', 'contentvec', 'contentvec_km100', 'contentvec_km500', 'cpc_local', 'cpc_url', 'customized_upstream', 'cvhubert', 'data2vec', 'data2vec_base_960']\n",
      "\n",
      "Will try to load upstream name: wavlm_large\n",
      "Loaded upstream: wavlm_large\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "cell_type": "markdown",
   "id": "eb40691027ae4b9a",
   "metadata": {},
   "source": [
    "## Extraction des embeddings et sauvegarde\n",
    "- On parcourt les fichiers audio rééchantillonnés\n",
    "- On charge chaque fichier audio\n",
    "- On extrait les embeddings via le modèle upstream\n",
    "- On sauvegarde les embeddings extraits"
   ]
  },
  {
   "cell_type": "code",
   "id": "4c23a37e20ddbac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:25:42.211534Z",
     "start_time": "2025-11-21T17:25:42.151062Z"
    }
   },
   "source": [
    "nb_treated_file = 0\n",
    "efnb = 0\n",
    "\n",
    "for file in tqdm(os.listdir(audio_path), desc=\"Extracting embeddings\"):\n",
    "    # 5) charge un wav mono 16kHz\n",
    "    wav_path = audio_path / file\n",
    "    if not wav_path.exists():\n",
    "        raise FileNotFoundError(\"Place a 16kHz WAV file in the current folder\")\n",
    "\n",
    "    if (efnb * 100.0 / fnb) % 10 == 0:\n",
    "        print(f\"Processed {efnb} files\")\n",
    "\n",
    "    # Sauvegarde path\n",
    "    save_path = embedding_path / (wav_path.stem + \".pt\")\n",
    "    if not save_path.exists():\n",
    "\n",
    "        nb_treated_file += 1\n",
    "\n",
    "        wav, sr = sf.read(str(wav_path))\n",
    "        if sr != 16000:\n",
    "            raise ValueError(f\"Le fichier doit être 16kHz. Fichier a {sr} Hz\")\n",
    "\n",
    "        # s3prl expects shape (batch, time)\n",
    "        wav_tensor = torch.from_numpy(wav).float().unsqueeze(0).to(device)\n",
    "\n",
    "        # length tensor (in samples)\n",
    "        wav_len = torch.LongTensor([wav_tensor.shape[1]]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # all_hs is a list of hidden states from different extraction points (depends on upstream)\n",
    "            all_hs, all_hs_len = upstream_model(wav_tensor, wav_len)\n",
    "\n",
    "        # Dernière couche (tu peux aussi prendre une moyenne, ex: torch.stack(all_hs[-4:]).mean(0))\n",
    "        emb = all_hs[-1].cpu().squeeze(0)  # (seq_len, hidden_dim)\n",
    "\n",
    "        torch.save(emb, save_path)\n",
    "    efnb += 1\n",
    "\n",
    "print(f\"Number of files processed: {nb_treated_file}\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 4240/4240 [00:00<00:00, 98232.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 files\n",
      "Processed 424 files\n",
      "Processed 848 files\n",
      "Processed 1272 files\n",
      "Processed 1696 files\n",
      "Processed 2120 files\n",
      "Processed 2544 files\n",
      "Processed 2968 files\n",
      "Processed 3392 files\n",
      "Processed 3816 files\n",
      "Number of files processed: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "cell_type": "markdown",
   "id": "d211411f8bc765d",
   "metadata": {},
   "source": [
    "## On pool les embeddings extraits pour classification\n",
    "- On definit les types de pooling à appliquer\n",
    "- On crée un petit modèle transformer pour l'agrégation séquentielle\n",
    "- On parcourt les fichiers d'embeddings extraits\n",
    "- On applique tous les types de pooling pour chaque embedding\n",
    "- On sauvegarde les embeddings formatés\n",
    "- Types de pooling :\n",
    "    - mean : moyenne (1024)\n",
    "    - max : maximum (1024)\n",
    "    - mean_std : concaténation de la moyenne et de l'écart-type (2048)\n",
    "    - transformer : agrégation via un petit modèle transformer (1024)"
   ]
  },
  {
   "cell_type": "code",
   "id": "e1aaaef6369bf182",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:25:46.905044Z",
     "start_time": "2025-11-21T17:25:42.215217Z"
    }
   },
   "source": [
    "# === Choix du type de pooling ===\n",
    "# Options possibles : \"mean\", \"max\", \"mean_std\"\n",
    "pooling_types = [\"mean\", \"max\", \"mean_std\"]\n",
    "\n",
    "# === Boucle sur les embeddings ===\n",
    "for file in tqdm(list(embedding_path.glob(\"*.pt\"))):\n",
    "\n",
    "    emb = torch.load(file)  # (seq_len, 1024)\n",
    "\n",
    "    for pooling_type in pooling_types:\n",
    "\n",
    "        save_path = pooled_path / pooling_type / f\"{file.stem}.pt\"\n",
    "\n",
    "        if not save_path.exists():\n",
    "\n",
    "            # === Pooling dynamique selon le choix utilisateur ===\n",
    "            if pooling_type == \"mean\":\n",
    "                emb_pooled = emb.mean(dim=0)  # (1024,)\n",
    "\n",
    "            elif pooling_type == \"max\":\n",
    "                emb_pooled, _ = emb.max(dim=0)  # (1024,)\n",
    "\n",
    "            elif pooling_type == \"mean_std\":\n",
    "                emb_mean = emb.mean(dim=0)\n",
    "                emb_std = emb.std(dim=0)\n",
    "                emb_pooled = torch.cat([emb_mean, emb_std])  # (2048,)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Type de pooling inconnu : {pooling_type}\")\n",
    "\n",
    "            # === Sauvegarde ===\n",
    "            torch.save(emb_pooled, save_path)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4240/4240 [00:04<00:00, 933.49it/s] \n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "cell_type": "markdown",
   "id": "d4c889fe5628a6c9",
   "metadata": {},
   "source": [
    "## Split des embeddings en sets d'entrainement, validation et test\n",
    "- On liste les chemins des fichiers d'embeddings\n",
    "- On split les fichiers en set d'entrainement, validation et test\n",
    "- On sauvegarde les splits"
   ]
  },
  {
   "cell_type": "code",
   "id": "7d4a78002a1888ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:25:46.973680Z",
     "start_time": "2025-11-21T17:25:46.931319Z"
    }
   },
   "source": [
    "# On va split les embedding en set pour les entrainements/validations/tests\n",
    "\n",
    "pooling_paths = [pooled_mean_path, pooled_max_path, pooled_mean_std_path]\n",
    "\n",
    "for path in pooling_paths:\n",
    "    files = os.listdir(path)\n",
    "\n",
    "    train_files, test_files = train_test_split(files, test_size=0.3, random_state=random_seed)  # 70% train, 30% test\n",
    "    print(\"Tain split is composed of \", len(train_files), \" files.\")\n",
    "    val_files, test_files = train_test_split(test_files, test_size=0.5,\n",
    "                                             random_state=random_seed)  # 50% val, 50% test donc 70% train 15% val 15% test\n",
    "    print(\"Validation split is composed of \", len(val_files), \" files.\")\n",
    "    print(\"Test split is composed of \", len(test_files), \" files.\\n\")\n",
    "\n",
    "    torch.save(train_files, path / \"_train.pt\")\n",
    "    torch.save(val_files, path / \"_val.pt\")\n",
    "    torch.save(test_files, path / \"_test.pt\")\n",
    "\n",
    "# Même chose pour les embeddings non poolés (transformer)\n",
    "\n",
    "files = os.listdir(embedding_path)\n",
    "\n",
    "train_files, test_files = train_test_split(files, test_size=0.3, random_state=random_seed)  # 70% train, 30% test\n",
    "print(\"Tain split is composed of \", len(train_files), \" files.\")\n",
    "val_files, test_files = train_test_split(test_files, test_size=0.5,\n",
    "                                         random_state=random_seed)  # 50% val, 50% test donc 70% train 15% val 15% test\n",
    "print(\"Validation split is composed of \", len(val_files), \" files.\")\n",
    "print(\"Test split is composed of \", len(test_files), \" files.\\n\")\n",
    "\n",
    "torch.save(train_files, transformer_path / \"_train.pt\")\n",
    "torch.save(val_files, transformer_path / \"_val.pt\")\n",
    "torch.save(test_files, transformer_path / \"_test.pt\")\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tain split is composed of  2970  files.\n",
      "Validation split is composed of  636  files.\n",
      "Test split is composed of  637  files.\n",
      "\n",
      "Tain split is composed of  2970  files.\n",
      "Validation split is composed of  636  files.\n",
      "Test split is composed of  637  files.\n",
      "\n",
      "Tain split is composed of  2970  files.\n",
      "Validation split is composed of  636  files.\n",
      "Test split is composed of  637  files.\n",
      "\n",
      "Tain split is composed of  2968  files.\n",
      "Validation split is composed of  636  files.\n",
      "Test split is composed of  637  files.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "cell_type": "markdown",
   "id": "57cc2ef4c6fdaaa7",
   "metadata": {},
   "source": [
    "## Vérification des fichiers sauvegardés\n",
    "On load un fichier du train set de la moyenne pour vérifier si on a bien sauvegardé correctement\n",
    "- On load le training set de la moyenne\n",
    "- On load un embedding de ce train set\n",
    "- On affiche la forme de l'embedding\n",
    "\n",
    "### Expected output\n",
    "```\n",
    "Number of training files (mean pooling): 72\n",
    "Loading embedding file: xxxxxxx.pt\n",
    "Embedding shape: torch.Size([1024])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "id": "c5766edc97b033ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:25:46.988658Z",
     "start_time": "2025-11-21T17:25:46.977716Z"
    }
   },
   "source": [
    "# On load un fichier du train set pour vérifier\n",
    "\n",
    "# On load le training sed de la moyenne\n",
    "train_files = torch.load(train_mean_path)\n",
    "transformer_train_files = torch.load(train_transformer_path)\n",
    "\n",
    "print(\"Number of training files (mean pooling):\", len(train_files))\n",
    "print(\"Number of training files (transformer):\", len(transformer_train_files))\n",
    "\n",
    "# On load un embedding de ce train set\n",
    "embedding_file = train_files[0]\n",
    "transformer_embedding_file = transformer_train_files[0]\n",
    "print(\"Loading embedding file:\", embedding_file)\n",
    "print(\"Loading transformer embedding file:\", transformer_embedding_file)\n",
    "\n",
    "embedding = torch.load(pooled_mean_path / embedding_file)\n",
    "transformer_embedding = torch.load(embedding_path / transformer_embedding_file)\n",
    "print(\"Embedding shape:\", embedding.shape)\n",
    "print(\"Transformer Embedding shape:\", transformer_embedding.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training files (mean pooling): 2970\n",
      "Number of training files (transformer): 2968\n",
      "Loading embedding file: OAF_peg_ps.pt\n",
      "Loading transformer embedding file: YAF_mill_angry.pt\n",
      "Embedding shape: torch.Size([1024])\n",
      "Transformer Embedding shape: torch.Size([105, 1024])\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "cell_type": "markdown",
   "id": "2fcb1be7f57f2b68",
   "metadata": {},
   "source": [
    "## Chargement des differents sets pour la methode de pooling renseignée et association des enregistrements à leur label"
   ]
  },
  {
   "cell_type": "code",
   "id": "b60b41812bcc7676",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:25:50.599143Z",
     "start_time": "2025-11-21T17:25:46.992624Z"
    }
   },
   "source": [
    "def load_split(file_list, base_path, is_transformer=False):\n",
    "    embeddings, labels = [], []\n",
    "    nb_par_classes = [0 for _ in range(8)]\n",
    "\n",
    "    for file_name in file_list:\n",
    "\n",
    "        if not file_name.startswith(\"_\") and file_name.endswith(\".pt\"):\n",
    "\n",
    "            # 1) Charge l'embedding\n",
    "            emb = torch.load(base_path / file_name)  # tensor\n",
    "\n",
    "            # 2) Détecte l'émotion\n",
    "            emo_code = find_emotion_T(file_name)\n",
    "            if emo_code == \"-1\":\n",
    "                emo_code = file_name[6:8]  # RAVDESS\n",
    "            label = emotionfix(emo_code)\n",
    "            nb_par_classes[label] += 1\n",
    "\n",
    "            embeddings.append(emb)\n",
    "            labels.append(label)\n",
    "    if is_transformer:\n",
    "        X = pad_sequence(embeddings, batch_first=True)\n",
    "    else:\n",
    "        X = torch.stack(embeddings)\n",
    "    y = torch.tensor(labels)\n",
    "    print(\"\\t - \",nb_par_classes)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "paths = {\n",
    "    \"mean\": [train_mean_path, val_mean_path, test_mean_path],\n",
    "    \"max\": [train_max_path, val_max_path, test_max_path],\n",
    "    \"mean_std\": [train_mean_std_path, val_mean_std_path, test_mean_std_path],\n",
    "    \"transformer\": [train_transformer_path, val_transformer_path, test_transformer_path]\n",
    "}\n",
    "\n",
    "pooling_type = \"mean\"  # \"max\", \"mean_std\", \"transformer\"\n",
    "\n",
    "train_path, val_path, test_path = paths[pooling_type]\n",
    "\n",
    "train_files = torch.load(train_path)\n",
    "val_files = torch.load(val_path)\n",
    "test_files = torch.load(test_path)\n",
    "\n",
    "print(\"Number of example for each class\\nTrain :\")\n",
    "X_train, y_train = load_split(train_files, pooled_mean_path)\n",
    "print(\"Val  :\")\n",
    "X_val, y_val = load_split(val_files, pooled_mean_path)\n",
    "print(\"Test  :\")\n",
    "X_test, y_test = load_split(test_files, pooled_mean_path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of example for each class\n",
      "Train :\n",
      "\t -  [351, 130, 415, 436, 403, 434, 400, 398]\n",
      "Val  :\n",
      "\t -  [80, 23, 101, 95, 89, 71, 93, 84]\n",
      "Test  :\n",
      "\t -  [65, 39, 76, 69, 98, 85, 97, 108]\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "cell_type": "markdown",
   "id": "6421035691f103a1",
   "metadata": {},
   "source": [
    "## Vérification des dimensions des sets"
   ]
  },
  {
   "cell_type": "code",
   "id": "8a18f0bd43992268",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:25:50.630109Z",
     "start_time": "2025-11-21T17:25:50.627684Z"
    }
   },
   "source": [
    "print(f\"[{pooling_type.upper()}] Loaded splits:\")\n",
    "print(\"Train :\", X_train.shape, y_train.shape, y_train.unique())\n",
    "print(\"Val   :\", X_val.shape, y_val.shape, y_val.unique())\n",
    "print(\"Test  :\", X_test.shape, y_test.shape, y_val.unique())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MEAN] Loaded splits:\n",
      "Train : torch.Size([2967, 1024]) torch.Size([2967]) tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "Val   : torch.Size([636, 1024]) torch.Size([636]) tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "Test  : torch.Size([637, 1024]) torch.Size([637]) tensor([0, 1, 2, 3, 4, 5, 6, 7])\n"
     ]
    }
   ],
   "execution_count": 73
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
